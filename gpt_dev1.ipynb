{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "## Building a GPT\n",
    "\n",
    "Companion notebook to the [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h5hjCcLDr2WC",
    "outputId": "ccc60f0c-fd78-4dbe-8598-0512d1036aad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-01-17 01:39:27--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
      "\n",
      "2023-01-17 01:39:28 (29.0 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "O6medjfRsLD9"
   },
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "with open(\"C:\\\\Users\\\\ncbir\\\\Desktop\\\\seqsmile.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xWI_VyAsN8F",
    "outputId": "ed819dd0-72e5-40a6-d2ed-928ff73bfda6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  77762536\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c5V0FvqseE0",
    "outputId": "25ca7adc-b8c0-42d1-b08c-e0863c5c314e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLLETQDALYVALELVIAALSVAGNVLVCAAVGTANTLQTPTNYFLVSLAAADVAVGLFAIPFAITISLGFCTDFYGCLFLACFVLVLTQSSIFSLLAVAVDRYLAICVPLRYKSLVTGTRARGVIAVLWVLAFGIGLTPFLGWNSKDSATNNCTEPWDGTTNESCCLVKCLFENVVPMSYMVYFNFFGCVLPPLLIMLVIYIKIFLVACRQLQRTELMDHSRTTLQREIHAAKSLAMIVGIFALCWLPVHAVNCVTLFQPAQGKNKPKWAMNMAILLSHANSVVNPIVYAYRNRDFRYTFHKIISRYLLCQADVKSGNGQAGVQPALGVGL COc1ccc(NC(=O)Nc2nc3ccc(Cl)cc3c3nc(nn23)-c2ccco2)cc1\n",
      "MKSILDGLADTTFRTITTDLLYVGSNDIQYEDIKGDMASKLGYFPQKFPLTSFRGSPFQEKMTAGDNSPLVPAGDTTNITEFYNKSLSSFKENEDNIQCGENFMDMECFMILNPSQQLAIAVLSLTLGTFTVLENLLVLCVILHSRSLRCRPSYHFIGSLAVADLLGSVIFVYSFVDFHVFHRKDSPNVFLFKLGGVTASFTASVGSLFLTAIDRYISIHRPLAYKRIVTRPKAVVAFCLMWTIAIVIAVLPLLGWNCKKLQSVCSDIFPLIDETYLMFWIGVTSVLLLFIVYAYMYILWKAHSHAVRMIQRGTQKSIIIHTSEDGKVQVTRPDQARMDIRLAKTLVLILVVLIICWGPLLAIMVYDVFGKMNKLIKTVFAFCSMLCLLNSTVNPIIYALRSKDLRHAFRSMFPSCEGTAQPLDNSMGDSDCLHKHANNTASMHRAAESCIKSTVKIAKVTMSVSTDTSAEAL Cc1ccc2c(c1)oc1c(nn(-c3ccc(Cl)cc3Cl)c21)C(=O)N[C@H]1C2(C)CCC(C2)C1(C)C\n",
      "MANSASPEQNQNHCSAINNSIPLMQGNLPTLTLSGKIRVTVTFFLFLLSATFNASFLLKLQKWTQKKEK\n"
     ]
    }
   ],
   "source": [
    "# let's look at the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e-Rbyr8sfM8",
    "outputId": "f34e94a9-5b44-4cf3-885b-986731929109"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " #()+-./123456789=@ABCDEFGHIKLMNOPQRSTUVWXYZ[\\]abcdefghiklmnopqrstuvwy\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yw1LKNCgwjj1",
    "outputId": "86fcc21c-2cf7-40d9-cd7b-b5a253da4459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 33, 50, 9, 50, 50, 50, 3, 32, 22, 3, 18, 33, 4, 32]\n",
      "COc1ccc(NC(=O)N\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"COc1ccc(NC(=O)N\"))\n",
    "print(decode(encode(\"COc1ccc(NC(=O)N\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '#': 2,\n",
       " '(': 3,\n",
       " ')': 4,\n",
       " '+': 5,\n",
       " '-': 6,\n",
       " '.': 7,\n",
       " '/': 8,\n",
       " '1': 9,\n",
       " '2': 10,\n",
       " '3': 11,\n",
       " '4': 12,\n",
       " '5': 13,\n",
       " '6': 14,\n",
       " '7': 15,\n",
       " '8': 16,\n",
       " '9': 17,\n",
       " '=': 18,\n",
       " '@': 19,\n",
       " 'A': 20,\n",
       " 'B': 21,\n",
       " 'C': 22,\n",
       " 'D': 23,\n",
       " 'E': 24,\n",
       " 'F': 25,\n",
       " 'G': 26,\n",
       " 'H': 27,\n",
       " 'I': 28,\n",
       " 'K': 29,\n",
       " 'L': 30,\n",
       " 'M': 31,\n",
       " 'N': 32,\n",
       " 'O': 33,\n",
       " 'P': 34,\n",
       " 'Q': 35,\n",
       " 'R': 36,\n",
       " 'S': 37,\n",
       " 'T': 38,\n",
       " 'U': 39,\n",
       " 'V': 40,\n",
       " 'W': 41,\n",
       " 'X': 42,\n",
       " 'Y': 43,\n",
       " 'Z': 44,\n",
       " '[': 45,\n",
       " '\\\\': 46,\n",
       " ']': 47,\n",
       " 'a': 48,\n",
       " 'b': 49,\n",
       " 'c': 50,\n",
       " 'd': 51,\n",
       " 'e': 52,\n",
       " 'f': 53,\n",
       " 'g': 54,\n",
       " 'h': 55,\n",
       " 'i': 56,\n",
       " 'k': 57,\n",
       " 'l': 58,\n",
       " 'm': 59,\n",
       " 'n': 60,\n",
       " 'o': 61,\n",
       " 'p': 62,\n",
       " 'q': 63,\n",
       " 'r': 64,\n",
       " 's': 65,\n",
       " 't': 66,\n",
       " 'u': 67,\n",
       " 'v': 68,\n",
       " 'w': 69,\n",
       " 'y': 70}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"C:\\\\Users\\\\ncbir\\\\Desktop\\\\dencode1.txt\"\n",
    "\n",
    "# Otwarcie pliku w trybie zapisu\n",
    "with open(file_path, 'w') as file:\n",
    "    # Iteracja przez elementy słownika i zapisanie ich do pliku\n",
    "    for key, value in itos.items():\n",
    "        file.write(f\"{key}: {value}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJb0OXPwzvqg",
    "outputId": "db7297cc-36a9-4fae-e941-e7bb9e0e91d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000000]) torch.int64\n",
      "tensor([31, 30, 30, 24, 38, 35, 23, 20, 30, 43, 40, 20, 30, 24, 30, 40, 28, 20,\n",
      "        20, 30, 37, 40, 20, 26, 32, 40, 30, 40, 22, 20, 20, 40, 26, 38, 20, 32,\n",
      "        38, 30, 35, 38, 34, 38, 32, 43, 25, 30, 40, 37, 30, 20, 20, 20, 23, 40,\n",
      "        20, 40, 26, 30, 25, 20, 28, 34, 25, 20, 28, 38, 28, 37, 30, 26, 25, 22,\n",
      "        38, 23, 25, 43, 26, 22, 30, 25, 30, 20, 22, 25, 40, 30, 40, 30, 38, 35,\n",
      "        37, 37, 28, 25, 37, 30, 30, 20, 40, 20, 40, 23, 36, 43, 30, 20, 28, 22,\n",
      "        40, 34, 30, 36, 43, 29, 37, 30, 40, 38, 26, 38, 36, 20, 36, 26, 40, 28,\n",
      "        20, 40, 30, 41, 40, 30, 20, 25, 26, 28, 26, 30, 38, 34, 25, 30, 26, 41,\n",
      "        32, 37, 29, 23, 37, 20, 38, 32, 32, 22, 38, 24, 34, 41, 23, 26, 38, 38,\n",
      "        32, 24, 37, 22, 22, 30, 40, 29, 22, 30, 25, 24, 32, 40, 40, 34, 31, 37,\n",
      "        43, 31, 40, 43, 25, 32, 25, 25, 26, 22, 40, 30, 34, 34, 30, 30, 28, 31,\n",
      "        30, 40, 28, 43, 28, 29, 28, 25, 30, 40, 20, 22, 36, 35, 30, 35, 36, 38,\n",
      "        24, 30, 31, 23, 27, 37, 36, 38, 38, 30, 35, 36, 24, 28, 27, 20, 20, 29,\n",
      "        37, 30, 20, 31, 28, 40, 26, 28, 25, 20, 30, 22, 41, 30, 34, 40, 27, 20,\n",
      "        40, 32, 22, 40, 38, 30, 25, 35, 34, 20, 35, 26, 29, 32, 29, 34, 29, 41,\n",
      "        20, 31, 32, 31, 20, 28, 30, 30, 37, 27, 20, 32, 37, 40, 40, 32, 34, 28,\n",
      "        40, 43, 20, 43, 36, 32, 36, 23, 25, 36, 43, 38, 25, 27, 29, 28, 28, 37,\n",
      "        36, 43, 30, 30, 22, 35, 20, 23, 40, 29, 37, 26, 32, 26, 35, 20, 26, 40,\n",
      "        35, 34, 20, 30, 26, 40, 26, 30,  1, 22, 33, 50,  9, 50, 50, 50,  3, 32,\n",
      "        22,  3, 18, 33,  4, 32, 50, 10, 60, 50, 11, 50, 50, 50,  3, 22, 58,  4,\n",
      "        50, 50, 11, 50, 11, 60, 50,  3, 60, 60, 10, 11,  4,  6, 50, 10, 50, 50,\n",
      "        50, 61, 10,  4, 50, 50,  9,  0, 31, 29, 37, 28, 30, 23, 26, 30, 20, 23,\n",
      "        38, 38, 25, 36, 38, 28, 38, 38, 23, 30, 30, 43, 40, 26, 37, 32, 23, 28,\n",
      "        35, 43, 24, 23, 28, 29, 26, 23, 31, 20, 37, 29, 30, 26, 43, 25, 34, 35,\n",
      "        29, 25, 34, 30, 38, 37, 25, 36, 26, 37, 34, 25, 35, 24, 29, 31, 38, 20,\n",
      "        26, 23, 32, 37, 34, 30, 40, 34, 20, 26, 23, 38, 38, 32, 28, 38, 24, 25,\n",
      "        43, 32, 29, 37, 30, 37, 37, 25, 29, 24, 32, 24, 23, 32, 28, 35, 22, 26,\n",
      "        24, 32, 25, 31, 23, 31, 24, 22, 25, 31, 28, 30, 32, 34, 37, 35, 35, 30,\n",
      "        20, 28, 20, 40, 30, 37, 30, 38, 30, 26, 38, 25, 38, 40, 30, 24, 32, 30,\n",
      "        30, 40, 30, 22, 40, 28, 30, 27, 37, 36, 37, 30, 36, 22, 36, 34, 37, 43,\n",
      "        27, 25, 28, 26, 37, 30, 20, 40, 20, 23, 30, 30, 26, 37, 40, 28, 25, 40,\n",
      "        43, 37, 25, 40, 23, 25, 27, 40, 25, 27, 36, 29, 23, 37, 34, 32, 40, 25,\n",
      "        30, 25, 29, 30, 26, 26, 40, 38, 20, 37, 25, 38, 20, 37, 40, 26, 37, 30,\n",
      "        25, 30, 38, 20, 28, 23, 36, 43, 28, 37, 28, 27, 36, 34, 30, 20, 43, 29,\n",
      "        36, 28, 40, 38, 36, 34, 29, 20, 40, 40, 20, 25, 22, 30, 31, 41, 38, 28,\n",
      "        20, 28, 40, 28, 20, 40, 30, 34, 30, 30, 26, 41, 32, 22, 29, 29, 30, 35,\n",
      "        37, 40, 22, 37, 23, 28, 25, 34, 30, 28, 23, 24, 38, 43, 30, 31, 25, 41,\n",
      "        28, 26, 40, 38, 37, 40, 30, 30, 30, 25, 28, 40, 43, 20, 43, 31, 43, 28,\n",
      "        30, 41, 29, 20, 27, 37, 27, 20, 40, 36, 31, 28, 35, 36, 26, 38, 35, 29,\n",
      "        37, 28, 28, 28, 27, 38, 37, 24, 23, 26, 29, 40, 35, 40, 38, 36, 34, 23,\n",
      "        35, 20, 36, 31, 23, 28, 36, 30, 20, 29, 38, 30, 40, 30, 28, 30, 40, 40,\n",
      "        30, 28, 28, 22, 41, 26, 34, 30, 30, 20, 28, 31, 40, 43, 23, 40, 25, 26,\n",
      "        29, 31, 32, 29, 30, 28, 29, 38, 40, 25, 20, 25, 22, 37, 31, 30, 22, 30,\n",
      "        30, 32, 37, 38, 40, 32, 34, 28, 28, 43, 20, 30, 36, 37, 29, 23, 30, 36,\n",
      "        27, 20, 25, 36, 37, 31, 25, 34, 37, 22, 24, 26, 38, 20, 35, 34, 30, 23,\n",
      "        32, 37, 31, 26, 23, 37, 23, 22, 30, 27, 29, 27, 20, 32, 32, 38, 20, 37,\n",
      "        31, 27, 36, 20, 20, 24, 37, 22, 28, 29, 37, 38, 40, 29, 28, 20, 29, 40,\n",
      "        38, 31, 37, 40, 37, 38, 23, 38, 37, 20, 24, 20, 30,  1, 22, 50,  9, 50,\n",
      "        50, 50, 10, 50,  3, 50,  9,  4, 61, 50,  9, 50,  3, 60, 60,  3,  6, 50,\n",
      "        11, 50, 50, 50,  3, 22, 58,  4, 50, 50, 11, 22, 58,  4, 50, 10,  9,  4,\n",
      "        22,  3, 18, 33,  4, 32, 45, 22, 19, 27, 47,  9, 22, 10,  3, 22,  4, 22,\n",
      "        22, 22,  3, 22, 10,  4, 22,  9,  3, 22,  4, 22,  0, 31, 20, 32, 37, 20,\n",
      "        37, 34, 24, 35, 32, 35, 32, 27, 22, 37, 20, 28, 32, 32, 37, 28, 34, 30,\n",
      "        31, 35, 26, 32, 30, 34, 38, 30, 38, 30, 37, 26, 29, 28, 36, 40, 38, 40,\n",
      "        38, 25, 25, 30, 25, 30, 30, 37, 20, 38, 25, 32, 20, 37, 25, 30, 30, 29,\n",
      "        30, 35, 29, 41, 38, 35, 29, 29, 24, 29])\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "import torch # we use PyTorch: https://pytorch.org\n",
    "data = torch.tensor(encode(text[:10000000]), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "f_WIXqxz0lU5"
   },
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TD5Bj8Y6IAD4",
    "outputId": "bf23c586-1d33-4af1-b63d-ce6f90b0a528"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([31, 30, 30, 24, 38, 35, 23, 20, 30])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(30)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HXDe8vGJCEn",
    "outputId": "588663aa-1de5-4ef7-aba0-4a96fe828353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([31]) the target: 30\n",
      "when input is tensor([31, 30]) the target: 30\n",
      "when input is tensor([31, 30, 30]) the target: 24\n",
      "when input is tensor([31, 30, 30, 24]) the target: 38\n",
      "when input is tensor([31, 30, 30, 24, 38]) the target: 35\n",
      "when input is tensor([31, 30, 30, 24, 38, 35]) the target: 23\n",
      "when input is tensor([31, 30, 30, 24, 38, 35, 23]) the target: 20\n",
      "when input is tensor([31, 30, 30, 24, 38, 35, 23, 20]) the target: 30\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q3k1Czf7LuA9",
    "outputId": "4ea8e8a0-443c-49bb-b3bf-ba36e1712999"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[38, 37, 25, 30, 37, 30, 40, 32],\n",
      "        [43, 32, 25, 30, 37, 26, 29, 25],\n",
      "        [43, 36, 25, 40, 38, 34, 26, 24],\n",
      "        [40, 35, 34, 28, 26, 34, 35, 38]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[37, 25, 30, 37, 30, 40, 32, 20],\n",
      "        [32, 25, 30, 37, 26, 29, 25, 36],\n",
      "        [36, 25, 40, 38, 34, 26, 24, 36],\n",
      "        [35, 34, 28, 26, 34, 35, 38, 34]])\n",
      "----\n",
      "when input is [38] the target: 37\n",
      "when input is [38, 37] the target: 25\n",
      "when input is [38, 37, 25] the target: 30\n",
      "when input is [38, 37, 25, 30] the target: 37\n",
      "when input is [38, 37, 25, 30, 37] the target: 30\n",
      "when input is [38, 37, 25, 30, 37, 30] the target: 40\n",
      "when input is [38, 37, 25, 30, 37, 30, 40] the target: 32\n",
      "when input is [38, 37, 25, 30, 37, 30, 40, 32] the target: 20\n",
      "when input is [43] the target: 32\n",
      "when input is [43, 32] the target: 25\n",
      "when input is [43, 32, 25] the target: 30\n",
      "when input is [43, 32, 25, 30] the target: 37\n",
      "when input is [43, 32, 25, 30, 37] the target: 26\n",
      "when input is [43, 32, 25, 30, 37, 26] the target: 29\n",
      "when input is [43, 32, 25, 30, 37, 26, 29] the target: 25\n",
      "when input is [43, 32, 25, 30, 37, 26, 29, 25] the target: 36\n",
      "when input is [43] the target: 36\n",
      "when input is [43, 36] the target: 25\n",
      "when input is [43, 36, 25] the target: 40\n",
      "when input is [43, 36, 25, 40] the target: 38\n",
      "when input is [43, 36, 25, 40, 38] the target: 34\n",
      "when input is [43, 36, 25, 40, 38, 34] the target: 26\n",
      "when input is [43, 36, 25, 40, 38, 34, 26] the target: 24\n",
      "when input is [43, 36, 25, 40, 38, 34, 26, 24] the target: 36\n",
      "when input is [40] the target: 35\n",
      "when input is [40, 35] the target: 34\n",
      "when input is [40, 35, 34] the target: 28\n",
      "when input is [40, 35, 34, 28] the target: 26\n",
      "when input is [40, 35, 34, 28, 26] the target: 34\n",
      "when input is [40, 35, 34, 28, 26, 34] the target: 35\n",
      "when input is [40, 35, 34, 28, 26, 34, 35] the target: 38\n",
      "when input is [40, 35, 34, 28, 26, 34, 35, 38] the target: 34\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qpyyAeIzQjlO",
    "outputId": "a650f8dc-da81-400b-bc59-0a595487fdb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[38, 37, 25, 30, 37, 30, 40, 32],\n",
      "        [43, 32, 25, 30, 37, 26, 29, 25],\n",
      "        [43, 36, 25, 40, 38, 34, 26, 24],\n",
      "        [40, 35, 34, 28, 26, 34, 35, 38]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # our input to the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nql_1ER53oCf",
    "outputId": "5de90b1b-4603-428a-f571-fe4bd3c45436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 71])\n",
      "tensor(4.7214, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "-r]Wwkf8TC)UuUbEkmTY8+Y7/aR@E oCB.Q7ba1HZl3lsGZ@si/aX/w8y//Q=95Q74lTsuI6mVr]1GZl3uUb.fLb5k+O=a/kC@i7\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "eTyJ8qAaDdiF"
   },
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs4kI8YdEkQj",
    "outputId": "42ded55c-2983-4d91-c528-675b2edfa849"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.6472978591918945\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(100): # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EcVIDWAZEtjN",
    "outputId": "0ad6f9d2-ad58-4498-a5f8-6f31407bb18b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gq8SLW]w8yrp\n",
      "MswvTgb5vC#.UA[AbPep.[MNilf 49ae@cIhSR9TTT\n",
      "Cgd(eqeWKCYSB.[2 72K6B2HPaeLnnOOvl@B3Lm-lIF\\DmV 1tGvUfTE22LMNDUZOmqZGbSGS\n",
      "s7ES/w#Bk+1y/lPew44GS#t54Q[Ni=nSGS/NQeQou6u\\\\K3DS9N\n",
      "cCp\\\\KT=Qa#]Vlw=oY1AEivrLRS\n",
      "pQ9D.gRrs47yhUV1G)#YHZh=2QdouiXqcbP2Z=FCBdV\\\\K2S7wu@rkqkCr]LI /Qe\n",
      "lylH[9+1fn(gb#iyvwdp.3nOm72hh\n",
      " laGB.(hQEewP- 1G5w.pVfSG7PlRsN#YUSbX3.31whZb46/\n",
      "=TZwv]\\\\VANEG47vwr]XPab(+P.oUT@\\Z.@LlRYprMM1v\n",
      "ymSw8+tT1dpDuRccS2CFAcCYKCLWpr]8/Mivf-34NNZG6K6F)p4K8@lHrcNu\n",
      "aHP4tKo[O]Zh9sv/wD.v7Sr8-ES91GAO p.()w\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XinV8nmAnmKN"
   },
   "source": [
    "## The mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tukiH-NbRBhA",
    "outputId": "d981f6d4-ac08-4ec2-8284-82f5fa1e0815"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs_E24uRE8kr",
    "outputId": "8bf3ff5f-565e-48b8-de8e-7272706c8e12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following toy example:\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86NuXX0fn7ps"
   },
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yhdOAd6-wXkZ",
    "outputId": "eaf6ab61-dff1-4bb7-e623-47f692bad5f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2: using matrix multiply for a weighted aggregation\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOURrfG-ysoL",
    "outputId": "080b500d-8110-4602-fcef-7d6f2ebfc6bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EDarxEWIRMKq",
    "outputId": "07b587dd-a91c-4bb0-d7f1-e247cd5dacb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vT1hdtzXCjgL",
    "outputId": "6d2c569b-7922-451f-9934-0fc564678d17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5CvobiQ0pLr"
   },
   "source": [
    "Notes:\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SNbLq5z3oBw"
   },
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nl6I9n9IRTSo",
    "outputId": "0c5b9cd0-af8a-4564-fbad-41d844e54822"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0449)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T1tQx7oeRvtc",
    "outputId": "3541ca1a-7447-4ef7-835e-81824aebc1b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0700)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MLb_odHU3iKM",
    "outputId": "a687a222-5a2c-4cdb-c1bf-17cd05b45b69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0918)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JB82yzt44REI",
    "outputId": "f07da2f1-10bb-4a7a-bcaa-578587977d00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mpt8569BB9_f",
    "outputId": "5d8b910a-6192-44ba-ebb2-497d88e0b629"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Num7sX9CKOH",
    "outputId": "929ceb78-a639-41d6-aac7-12997b5c93f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "\n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # batch mean\n",
    "    xvar = x.var(1, keepdim=True) # batch variance\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "633T2cmnW1uk",
    "outputId": "7720fa58-0478-4e8a-86a7-502d4cce9443"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1469), tensor(0.8803))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LN9cK9BoXCYb",
    "outputId": "6368ece0-600e-417d-8a91-7c1e5d750ba8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-9.5367e-09), tensor(1.0000))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dRJH6wM_XFfU"
   },
   "outputs": [],
   "source": [
    "# French to English translation example:\n",
    "\n",
    "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
    "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcvKeBXoZFOY"
   },
   "source": [
    "### Full finished code, for reference\n",
    "\n",
    "You may want to refer directly to the git repo instead though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoelkOrFY8bN",
    "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.210503 M parameters\n",
      "step 0: train loss 4.4041, val loss 4.4016\n",
      "step 100: train loss 2.9087, val loss 2.9048\n",
      "step 200: train loss 2.8039, val loss 2.8013\n",
      "step 300: train loss 2.7746, val loss 2.7504\n",
      "step 400: train loss 2.7381, val loss 2.7471\n",
      "step 500: train loss 2.7265, val loss 2.7373\n",
      "step 600: train loss 2.7100, val loss 2.7062\n",
      "step 700: train loss 2.7164, val loss 2.7027\n",
      "step 800: train loss 2.7103, val loss 2.7016\n",
      "step 900: train loss 2.6927, val loss 2.6813\n",
      "step 1000: train loss 2.6951, val loss 2.6824\n",
      "step 1100: train loss 2.6733, val loss 2.6682\n",
      "step 1200: train loss 2.6790, val loss 2.6766\n",
      "step 1300: train loss 2.6572, val loss 2.6648\n",
      "step 1400: train loss 2.6519, val loss 2.6580\n",
      "step 1500: train loss 2.6468, val loss 2.6335\n",
      "step 1600: train loss 2.6555, val loss 2.6294\n",
      "step 1700: train loss 2.6276, val loss 2.6387\n",
      "step 1800: train loss 2.6360, val loss 2.6174\n",
      "step 1900: train loss 2.6206, val loss 2.6125\n",
      "step 2000: train loss 2.6119, val loss 2.6051\n",
      "step 2100: train loss 2.6151, val loss 2.6042\n",
      "step 2200: train loss 2.5763, val loss 2.5832\n",
      "step 2300: train loss 2.5827, val loss 2.5758\n",
      "step 2400: train loss 2.5695, val loss 2.5837\n",
      "step 2500: train loss 2.5593, val loss 2.5574\n",
      "step 2600: train loss 2.5631, val loss 2.5437\n",
      "step 2700: train loss 2.5389, val loss 2.5418\n",
      "step 2800: train loss 2.5284, val loss 2.5144\n",
      "step 2900: train loss 2.5167, val loss 2.5118\n",
      "step 3000: train loss 2.5388, val loss 2.5249\n",
      "step 3100: train loss 2.5026, val loss 2.4861\n",
      "step 3200: train loss 2.5013, val loss 2.4833\n",
      "step 3300: train loss 2.4819, val loss 2.4751\n",
      "step 3400: train loss 2.4579, val loss 2.4453\n",
      "step 3500: train loss 2.4440, val loss 2.4538\n",
      "step 3600: train loss 2.4242, val loss 2.4504\n",
      "step 3700: train loss 2.4256, val loss 2.4344\n",
      "step 3800: train loss 2.4046, val loss 2.4286\n",
      "step 3900: train loss 2.4135, val loss 2.3974\n",
      "step 4000: train loss 2.4008, val loss 2.4039\n",
      "step 4100: train loss 2.4023, val loss 2.4022\n",
      "step 4200: train loss 2.3772, val loss 2.3650\n",
      "step 4300: train loss 2.3570, val loss 2.3609\n",
      "step 4400: train loss 2.3654, val loss 2.3505\n",
      "step 4500: train loss 2.3578, val loss 2.3360\n",
      "step 4600: train loss 2.3332, val loss 2.3210\n",
      "step 4700: train loss 2.3256, val loss 2.3066\n",
      "step 4800: train loss 2.3151, val loss 2.2960\n",
      "step 4900: train loss 2.3056, val loss 2.2717\n",
      "step 5000: train loss 2.2972, val loss 2.2910\n",
      "step 5100: train loss 2.2589, val loss 2.2703\n",
      "step 5200: train loss 2.2761, val loss 2.2554\n",
      "step 5300: train loss 2.2645, val loss 2.2721\n",
      "step 5400: train loss 2.2556, val loss 2.2428\n",
      "step 5500: train loss 2.2436, val loss 2.2321\n",
      "step 5600: train loss 2.2089, val loss 2.2295\n",
      "step 5700: train loss 2.2314, val loss 2.2099\n",
      "step 5800: train loss 2.2138, val loss 2.2015\n",
      "step 5900: train loss 2.2180, val loss 2.2139\n",
      "step 6000: train loss 2.2053, val loss 2.2194\n",
      "step 6100: train loss 2.1622, val loss 2.1788\n",
      "step 6200: train loss 2.1842, val loss 2.1899\n",
      "step 6300: train loss 2.1752, val loss 2.1450\n",
      "step 6400: train loss 2.1610, val loss 2.1592\n",
      "step 6500: train loss 2.1568, val loss 2.1373\n",
      "step 6600: train loss 2.1415, val loss 2.1505\n",
      "step 6700: train loss 2.1280, val loss 2.1485\n",
      "step 6800: train loss 2.1515, val loss 2.1273\n",
      "step 6900: train loss 2.1345, val loss 2.1009\n",
      "step 7000: train loss 2.1315, val loss 2.1422\n",
      "step 7100: train loss 2.1213, val loss 2.0923\n",
      "step 7200: train loss 2.0969, val loss 2.1029\n",
      "step 7300: train loss 2.0871, val loss 2.0861\n",
      "step 7400: train loss 2.0722, val loss 2.0657\n",
      "step 7500: train loss 2.0845, val loss 2.0816\n",
      "step 7600: train loss 2.0544, val loss 2.0699\n",
      "step 7700: train loss 2.0530, val loss 2.0603\n",
      "step 7800: train loss 2.0625, val loss 2.0626\n",
      "step 7900: train loss 2.0687, val loss 2.0426\n",
      "step 8000: train loss 2.0282, val loss 2.0275\n",
      "step 8100: train loss 2.0276, val loss 2.0173\n",
      "step 8200: train loss 2.0371, val loss 2.0225\n",
      "step 8300: train loss 2.0368, val loss 2.0013\n",
      "step 8400: train loss 2.0579, val loss 1.9817\n",
      "step 8500: train loss 2.0232, val loss 2.0284\n",
      "step 8600: train loss 2.0269, val loss 2.0325\n",
      "step 8700: train loss 2.0110, val loss 1.9912\n",
      "step 8800: train loss 1.9945, val loss 2.0054\n",
      "step 8900: train loss 1.9978, val loss 1.9916\n",
      "step 9000: train loss 1.9773, val loss 1.9896\n",
      "step 9100: train loss 1.9989, val loss 1.9949\n",
      "step 9200: train loss 1.9843, val loss 1.9632\n",
      "step 9300: train loss 1.9523, val loss 1.9788\n",
      "step 9400: train loss 1.9554, val loss 1.9812\n",
      "step 9500: train loss 1.9722, val loss 1.9535\n",
      "step 9600: train loss 1.9706, val loss 1.9837\n",
      "step 9700: train loss 1.9592, val loss 1.9683\n",
      "step 9800: train loss 1.9566, val loss 1.9364\n",
      "step 9900: train loss 1.9337, val loss 1.9299\n",
      "step 10000: train loss 1.9467, val loss 1.9418\n",
      "step 10100: train loss 1.9437, val loss 1.9621\n",
      "step 10200: train loss 1.8914, val loss 1.9179\n",
      "step 10300: train loss 1.9414, val loss 1.9081\n",
      "step 10400: train loss 1.9698, val loss 1.9418\n",
      "step 10500: train loss 1.8786, val loss 1.9244\n",
      "step 10600: train loss 1.9154, val loss 1.8987\n",
      "step 10700: train loss 1.9219, val loss 1.8962\n",
      "step 10800: train loss 1.9220, val loss 1.9017\n",
      "step 10900: train loss 1.9015, val loss 1.8998\n",
      "step 11000: train loss 1.8925, val loss 1.8870\n",
      "step 11100: train loss 1.9059, val loss 1.8843\n",
      "step 11200: train loss 1.8961, val loss 1.8815\n",
      "step 11300: train loss 1.9034, val loss 1.8714\n",
      "step 11400: train loss 1.8526, val loss 1.8466\n",
      "step 11500: train loss 1.8739, val loss 1.8496\n",
      "step 11600: train loss 1.8644, val loss 1.8807\n",
      "step 11700: train loss 1.8777, val loss 1.8587\n",
      "step 11800: train loss 1.8751, val loss 1.8861\n",
      "step 11900: train loss 1.8487, val loss 1.8219\n",
      "step 12000: train loss 1.8752, val loss 1.8528\n",
      "step 12100: train loss 1.8219, val loss 1.8425\n",
      "step 12200: train loss 1.8582, val loss 1.8458\n",
      "step 12300: train loss 1.8346, val loss 1.8209\n",
      "step 12400: train loss 1.8296, val loss 1.8246\n",
      "step 12500: train loss 1.8196, val loss 1.8210\n",
      "step 12600: train loss 1.8018, val loss 1.8303\n",
      "step 12700: train loss 1.8255, val loss 1.8371\n",
      "step 12800: train loss 1.8293, val loss 1.8284\n",
      "step 12900: train loss 1.7690, val loss 1.8084\n",
      "step 13000: train loss 1.8270, val loss 1.8095\n",
      "step 13100: train loss 1.8109, val loss 1.8141\n",
      "step 13200: train loss 1.8241, val loss 1.7761\n",
      "step 13300: train loss 1.7817, val loss 1.7880\n",
      "step 13400: train loss 1.8082, val loss 1.7848\n",
      "step 13500: train loss 1.7965, val loss 1.8095\n",
      "step 13600: train loss 1.8250, val loss 1.7824\n",
      "step 13700: train loss 1.7790, val loss 1.7818\n",
      "step 13800: train loss 1.7962, val loss 1.7916\n",
      "step 13900: train loss 1.7957, val loss 1.7948\n",
      "step 14000: train loss 1.7918, val loss 1.7581\n",
      "step 14100: train loss 1.8050, val loss 1.7965\n",
      "step 14200: train loss 1.7676, val loss 1.7367\n",
      "step 14300: train loss 1.7837, val loss 1.7949\n",
      "step 14400: train loss 1.7619, val loss 1.7685\n",
      "step 14500: train loss 1.8167, val loss 1.7863\n",
      "step 14600: train loss 1.7768, val loss 1.7847\n",
      "step 14700: train loss 1.7645, val loss 1.7691\n",
      "step 14800: train loss 1.7635, val loss 1.7540\n",
      "step 14900: train loss 1.7641, val loss 1.7545\n",
      "step 15000: train loss 1.7828, val loss 1.7586\n",
      "step 15100: train loss 1.7486, val loss 1.7460\n",
      "step 15200: train loss 1.7547, val loss 1.7431\n",
      "step 15300: train loss 1.7647, val loss 1.7566\n",
      "step 15400: train loss 1.7460, val loss 1.7502\n",
      "step 15500: train loss 1.7686, val loss 1.7204\n",
      "step 15600: train loss 1.7180, val loss 1.7353\n",
      "step 15700: train loss 1.7074, val loss 1.7451\n",
      "step 15800: train loss 1.7610, val loss 1.7646\n",
      "step 15900: train loss 1.7555, val loss 1.7186\n",
      "step 16000: train loss 1.7081, val loss 1.6784\n",
      "step 16100: train loss 1.7321, val loss 1.7207\n",
      "step 16200: train loss 1.7058, val loss 1.7186\n",
      "step 16300: train loss 1.7317, val loss 1.7130\n",
      "step 16400: train loss 1.7210, val loss 1.7056\n",
      "step 16500: train loss 1.7030, val loss 1.7006\n",
      "step 16600: train loss 1.7261, val loss 1.7021\n",
      "step 16700: train loss 1.7240, val loss 1.7008\n",
      "step 16800: train loss 1.7404, val loss 1.7034\n",
      "step 16900: train loss 1.7176, val loss 1.7067\n",
      "step 17000: train loss 1.7054, val loss 1.7244\n",
      "step 17100: train loss 1.6964, val loss 1.7009\n",
      "step 17200: train loss 1.7066, val loss 1.7010\n",
      "step 17300: train loss 1.7068, val loss 1.7108\n",
      "step 17400: train loss 1.7314, val loss 1.7068\n",
      "step 17500: train loss 1.7064, val loss 1.6884\n",
      "step 17600: train loss 1.7204, val loss 1.7085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 17700: train loss 1.6762, val loss 1.6695\n",
      "step 17800: train loss 1.7009, val loss 1.6900\n",
      "step 17900: train loss 1.6693, val loss 1.7130\n",
      "step 18000: train loss 1.6750, val loss 1.6770\n",
      "step 18100: train loss 1.6304, val loss 1.6380\n",
      "step 18200: train loss 1.7047, val loss 1.6595\n",
      "step 18300: train loss 1.6872, val loss 1.6875\n",
      "step 18400: train loss 1.6616, val loss 1.6575\n",
      "step 18500: train loss 1.6847, val loss 1.6670\n",
      "step 18600: train loss 1.6446, val loss 1.6336\n",
      "step 18700: train loss 1.7310, val loss 1.6696\n",
      "step 18800: train loss 1.6649, val loss 1.6601\n",
      "step 18900: train loss 1.6795, val loss 1.6467\n",
      "step 19000: train loss 1.6669, val loss 1.6615\n",
      "step 19100: train loss 1.6519, val loss 1.6334\n",
      "step 19200: train loss 1.6407, val loss 1.6962\n",
      "step 19300: train loss 1.6619, val loss 1.6479\n",
      "step 19400: train loss 1.6993, val loss 1.6685\n",
      "step 19500: train loss 1.6254, val loss 1.6489\n",
      "step 19600: train loss 1.6742, val loss 1.6483\n",
      "step 19700: train loss 1.6640, val loss 1.6600\n",
      "step 19800: train loss 1.6317, val loss 1.6211\n",
      "step 19900: train loss 1.6354, val loss 1.6381\n",
      "step 20000: train loss 1.6825, val loss 1.6581\n",
      "step 20100: train loss 1.6470, val loss 1.6374\n",
      "step 20200: train loss 1.6523, val loss 1.6490\n",
      "step 20300: train loss 1.6374, val loss 1.6153\n",
      "step 20400: train loss 1.6367, val loss 1.6192\n",
      "step 20500: train loss 1.6338, val loss 1.6181\n",
      "step 20600: train loss 1.6176, val loss 1.6396\n",
      "step 20700: train loss 1.6526, val loss 1.6382\n",
      "step 20800: train loss 1.6081, val loss 1.5895\n",
      "step 20900: train loss 1.6203, val loss 1.6714\n",
      "step 21000: train loss 1.6251, val loss 1.6339\n",
      "step 21100: train loss 1.6070, val loss 1.6080\n",
      "step 21200: train loss 1.6331, val loss 1.6187\n",
      "step 21300: train loss 1.6277, val loss 1.6383\n",
      "step 21400: train loss 1.6389, val loss 1.6009\n",
      "step 21500: train loss 1.6303, val loss 1.6121\n",
      "step 21600: train loss 1.6465, val loss 1.6423\n",
      "step 21700: train loss 1.6322, val loss 1.6434\n",
      "step 21800: train loss 1.5985, val loss 1.6243\n",
      "step 21900: train loss 1.5878, val loss 1.5741\n",
      "step 22000: train loss 1.6065, val loss 1.6334\n",
      "step 22100: train loss 1.6253, val loss 1.6034\n",
      "step 22200: train loss 1.6006, val loss 1.6315\n",
      "step 22300: train loss 1.6127, val loss 1.6381\n",
      "step 22400: train loss 1.6211, val loss 1.5819\n",
      "step 22500: train loss 1.5836, val loss 1.6049\n",
      "step 22600: train loss 1.6233, val loss 1.5953\n",
      "step 22700: train loss 1.5634, val loss 1.6084\n",
      "step 22800: train loss 1.5669, val loss 1.6191\n",
      "step 22900: train loss 1.5932, val loss 1.5796\n",
      "step 23000: train loss 1.6169, val loss 1.6228\n",
      "step 23100: train loss 1.5762, val loss 1.5858\n",
      "step 23200: train loss 1.6248, val loss 1.5895\n",
      "step 23300: train loss 1.5970, val loss 1.5760\n",
      "step 23400: train loss 1.5882, val loss 1.6164\n",
      "step 23500: train loss 1.5948, val loss 1.6022\n",
      "step 23600: train loss 1.5750, val loss 1.5701\n",
      "step 23700: train loss 1.5586, val loss 1.5729\n",
      "step 23800: train loss 1.5651, val loss 1.5886\n",
      "step 23900: train loss 1.5403, val loss 1.5589\n",
      "step 24000: train loss 1.6010, val loss 1.5669\n",
      "step 24100: train loss 1.5655, val loss 1.6123\n",
      "step 24200: train loss 1.5774, val loss 1.5760\n",
      "step 24300: train loss 1.5597, val loss 1.5508\n",
      "step 24400: train loss 1.5697, val loss 1.5873\n",
      "step 24500: train loss 1.5530, val loss 1.5840\n",
      "step 24600: train loss 1.5486, val loss 1.5772\n",
      "step 24700: train loss 1.5511, val loss 1.5939\n",
      "step 24800: train loss 1.5928, val loss 1.5864\n",
      "step 24900: train loss 1.5512, val loss 1.5644\n",
      "step 25000: train loss 1.5682, val loss 1.5604\n",
      "step 25100: train loss 1.5704, val loss 1.5246\n",
      "step 25200: train loss 1.5441, val loss 1.5744\n",
      "step 25300: train loss 1.6081, val loss 1.5481\n",
      "step 25400: train loss 1.5715, val loss 1.5589\n",
      "step 25500: train loss 1.5629, val loss 1.5613\n",
      "step 25600: train loss 1.5701, val loss 1.5712\n",
      "step 25700: train loss 1.5632, val loss 1.5548\n",
      "step 25800: train loss 1.5340, val loss 1.5597\n",
      "step 25900: train loss 1.5715, val loss 1.5676\n",
      "step 26000: train loss 1.5704, val loss 1.5378\n",
      "step 26100: train loss 1.5613, val loss 1.5318\n",
      "step 26200: train loss 1.5444, val loss 1.5757\n",
      "step 26300: train loss 1.5889, val loss 1.5448\n",
      "step 26400: train loss 1.5303, val loss 1.5428\n",
      "step 26500: train loss 1.5676, val loss 1.5491\n",
      "step 26600: train loss 1.5447, val loss 1.5387\n",
      "step 26700: train loss 1.5323, val loss 1.5593\n",
      "step 26800: train loss 1.5693, val loss 1.5593\n",
      "step 26900: train loss 1.5714, val loss 1.5435\n",
      "step 27000: train loss 1.5634, val loss 1.5563\n",
      "step 27100: train loss 1.5352, val loss 1.5464\n",
      "step 27200: train loss 1.5406, val loss 1.5493\n",
      "step 27300: train loss 1.5649, val loss 1.5504\n",
      "step 27400: train loss 1.5378, val loss 1.5246\n",
      "step 27500: train loss 1.5785, val loss 1.5506\n",
      "step 27600: train loss 1.5407, val loss 1.5059\n",
      "step 27700: train loss 1.4990, val loss 1.5278\n",
      "step 27800: train loss 1.5255, val loss 1.5504\n",
      "step 27900: train loss 1.5113, val loss 1.5448\n",
      "step 28000: train loss 1.5294, val loss 1.5262\n",
      "step 28100: train loss 1.5403, val loss 1.5293\n",
      "step 28200: train loss 1.5436, val loss 1.5140\n",
      "step 28300: train loss 1.5296, val loss 1.5219\n",
      "step 28400: train loss 1.5323, val loss 1.5363\n",
      "step 28500: train loss 1.5401, val loss 1.5107\n",
      "step 28600: train loss 1.5411, val loss 1.5194\n",
      "step 28700: train loss 1.5572, val loss 1.5114\n",
      "step 28800: train loss 1.5432, val loss 1.5246\n",
      "step 28900: train loss 1.5095, val loss 1.5172\n",
      "step 29000: train loss 1.5035, val loss 1.5408\n",
      "step 29100: train loss 1.5498, val loss 1.5111\n",
      "step 29200: train loss 1.5277, val loss 1.5305\n",
      "step 29300: train loss 1.4865, val loss 1.4771\n",
      "step 29400: train loss 1.5084, val loss 1.5285\n",
      "step 29500: train loss 1.5150, val loss 1.5134\n",
      "step 29600: train loss 1.5349, val loss 1.5161\n",
      "step 29700: train loss 1.5432, val loss 1.5378\n",
      "step 29800: train loss 1.5185, val loss 1.5099\n",
      "step 29900: train loss 1.4910, val loss 1.5009\n",
      "step 30000: train loss 1.5410, val loss 1.5176\n",
      "step 30100: train loss 1.5467, val loss 1.5419\n",
      "step 30200: train loss 1.5574, val loss 1.4933\n",
      "step 30300: train loss 1.5119, val loss 1.5046\n",
      "step 30400: train loss 1.5128, val loss 1.4959\n",
      "step 30500: train loss 1.4634, val loss 1.5001\n",
      "step 30600: train loss 1.5375, val loss 1.5037\n",
      "step 30700: train loss 1.4820, val loss 1.5321\n",
      "step 30800: train loss 1.5033, val loss 1.5202\n",
      "step 30900: train loss 1.5550, val loss 1.5270\n",
      "step 31000: train loss 1.5113, val loss 1.5038\n",
      "step 31100: train loss 1.5401, val loss 1.5186\n",
      "step 31200: train loss 1.5507, val loss 1.5027\n",
      "step 31300: train loss 1.5031, val loss 1.5179\n",
      "step 31400: train loss 1.5136, val loss 1.5122\n",
      "step 31500: train loss 1.4810, val loss 1.5002\n",
      "step 31600: train loss 1.5129, val loss 1.4924\n",
      "step 31700: train loss 1.5131, val loss 1.4899\n",
      "step 31800: train loss 1.5101, val loss 1.4778\n",
      "step 31900: train loss 1.5022, val loss 1.5045\n",
      "step 32000: train loss 1.5137, val loss 1.4816\n",
      "step 32100: train loss 1.5265, val loss 1.5021\n",
      "step 32200: train loss 1.4909, val loss 1.5127\n",
      "step 32300: train loss 1.5214, val loss 1.4979\n",
      "step 32400: train loss 1.5088, val loss 1.4731\n",
      "step 32500: train loss 1.4830, val loss 1.4594\n",
      "step 32600: train loss 1.5152, val loss 1.4906\n",
      "step 32700: train loss 1.4971, val loss 1.5087\n",
      "step 32800: train loss 1.5179, val loss 1.5029\n",
      "step 32900: train loss 1.5131, val loss 1.4856\n",
      "step 33000: train loss 1.5023, val loss 1.4609\n",
      "step 33100: train loss 1.5215, val loss 1.4715\n",
      "step 33200: train loss 1.5018, val loss 1.4848\n",
      "step 33300: train loss 1.4874, val loss 1.4650\n",
      "step 33400: train loss 1.4534, val loss 1.5140\n",
      "step 33500: train loss 1.4880, val loss 1.4924\n",
      "step 33600: train loss 1.4867, val loss 1.5066\n",
      "step 33700: train loss 1.4809, val loss 1.4764\n",
      "step 33800: train loss 1.4698, val loss 1.4702\n",
      "step 33900: train loss 1.4912, val loss 1.4977\n",
      "step 34000: train loss 1.4853, val loss 1.4637\n",
      "step 34100: train loss 1.4690, val loss 1.4837\n",
      "step 34200: train loss 1.4868, val loss 1.4747\n",
      "step 34300: train loss 1.4643, val loss 1.4740\n",
      "step 34400: train loss 1.4996, val loss 1.4662\n",
      "step 34500: train loss 1.4932, val loss 1.5021\n",
      "step 34600: train loss 1.4917, val loss 1.4889\n",
      "step 34700: train loss 1.5110, val loss 1.4714\n",
      "step 34800: train loss 1.4747, val loss 1.4836\n",
      "step 34900: train loss 1.4703, val loss 1.5111\n",
      "step 35000: train loss 1.5068, val loss 1.4747\n",
      "step 35100: train loss 1.4987, val loss 1.4865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 35200: train loss 1.4917, val loss 1.4944\n",
      "step 35300: train loss 1.4827, val loss 1.4918\n",
      "step 35400: train loss 1.4950, val loss 1.4655\n",
      "step 35500: train loss 1.4860, val loss 1.4737\n",
      "step 35600: train loss 1.4801, val loss 1.4668\n",
      "step 35700: train loss 1.4758, val loss 1.4727\n",
      "step 35800: train loss 1.4562, val loss 1.4293\n",
      "step 35900: train loss 1.5195, val loss 1.4917\n",
      "step 36000: train loss 1.4879, val loss 1.4844\n",
      "step 36100: train loss 1.4982, val loss 1.4572\n",
      "step 36200: train loss 1.5127, val loss 1.4562\n",
      "step 36300: train loss 1.4878, val loss 1.4958\n",
      "step 36400: train loss 1.4697, val loss 1.4439\n",
      "step 36500: train loss 1.4905, val loss 1.4861\n",
      "step 36600: train loss 1.4797, val loss 1.4528\n",
      "step 36700: train loss 1.4618, val loss 1.4527\n",
      "step 36800: train loss 1.4642, val loss 1.4545\n",
      "step 36900: train loss 1.4959, val loss 1.4356\n",
      "step 37000: train loss 1.4851, val loss 1.4578\n",
      "step 37100: train loss 1.4921, val loss 1.4706\n",
      "step 37200: train loss 1.4511, val loss 1.4647\n",
      "step 37300: train loss 1.4770, val loss 1.4636\n",
      "step 37400: train loss 1.4483, val loss 1.4709\n",
      "step 37500: train loss 1.4401, val loss 1.4763\n",
      "step 37600: train loss 1.4336, val loss 1.4704\n",
      "step 37700: train loss 1.5002, val loss 1.4692\n",
      "step 37800: train loss 1.4703, val loss 1.4750\n",
      "step 37900: train loss 1.4386, val loss 1.4526\n",
      "step 38000: train loss 1.5028, val loss 1.4527\n",
      "step 38100: train loss 1.4753, val loss 1.4691\n",
      "step 38200: train loss 1.4895, val loss 1.4754\n",
      "step 38300: train loss 1.4495, val loss 1.4534\n",
      "step 38400: train loss 1.4656, val loss 1.4383\n",
      "step 38500: train loss 1.4792, val loss 1.4470\n",
      "step 38600: train loss 1.4657, val loss 1.4538\n",
      "step 38700: train loss 1.4679, val loss 1.4773\n",
      "step 38800: train loss 1.4346, val loss 1.4530\n",
      "step 38900: train loss 1.4987, val loss 1.4601\n",
      "step 39000: train loss 1.4691, val loss 1.4644\n",
      "step 39100: train loss 1.4352, val loss 1.4414\n",
      "step 39200: train loss 1.4704, val loss 1.4355\n",
      "step 39300: train loss 1.4837, val loss 1.4380\n",
      "step 39400: train loss 1.4886, val loss 1.4763\n",
      "step 39500: train loss 1.4748, val loss 1.4890\n",
      "step 39600: train loss 1.4401, val loss 1.4385\n",
      "step 39700: train loss 1.4892, val loss 1.4701\n",
      "step 39800: train loss 1.4467, val loss 1.4477\n",
      "step 39900: train loss 1.4343, val loss 1.4890\n",
      "step 40000: train loss 1.4656, val loss 1.4423\n",
      "step 40100: train loss 1.4638, val loss 1.4146\n",
      "step 40200: train loss 1.4429, val loss 1.4377\n",
      "step 40300: train loss 1.4596, val loss 1.4212\n",
      "step 40400: train loss 1.4175, val loss 1.4431\n",
      "step 40500: train loss 1.4386, val loss 1.4514\n",
      "step 40600: train loss 1.4406, val loss 1.4251\n",
      "step 40700: train loss 1.4539, val loss 1.4594\n",
      "step 40800: train loss 1.4271, val loss 1.4748\n",
      "step 40900: train loss 1.4076, val loss 1.4307\n",
      "step 41000: train loss 1.4645, val loss 1.4074\n",
      "step 41100: train loss 1.4606, val loss 1.4348\n",
      "step 41200: train loss 1.4719, val loss 1.4818\n",
      "step 41300: train loss 1.4950, val loss 1.4328\n",
      "step 41400: train loss 1.4125, val loss 1.4208\n",
      "step 41500: train loss 1.4116, val loss 1.4471\n",
      "step 41600: train loss 1.4632, val loss 1.4244\n",
      "step 41700: train loss 1.4552, val loss 1.4642\n",
      "step 41800: train loss 1.4335, val loss 1.4142\n",
      "step 41900: train loss 1.4367, val loss 1.4271\n",
      "step 42000: train loss 1.4289, val loss 1.4387\n",
      "step 42100: train loss 1.4310, val loss 1.4220\n",
      "step 42200: train loss 1.4578, val loss 1.4474\n",
      "step 42300: train loss 1.4491, val loss 1.4542\n",
      "step 42400: train loss 1.4227, val loss 1.4181\n",
      "step 42500: train loss 1.4429, val loss 1.4308\n",
      "step 42600: train loss 1.4649, val loss 1.4430\n",
      "step 42700: train loss 1.4322, val loss 1.4249\n",
      "step 42800: train loss 1.4310, val loss 1.4235\n",
      "step 42900: train loss 1.4175, val loss 1.4257\n",
      "step 43000: train loss 1.4234, val loss 1.4226\n",
      "step 43100: train loss 1.4378, val loss 1.4565\n",
      "step 43200: train loss 1.4158, val loss 1.4367\n",
      "step 43300: train loss 1.4305, val loss 1.4435\n",
      "step 43400: train loss 1.4697, val loss 1.4330\n",
      "step 43500: train loss 1.4254, val loss 1.4348\n",
      "step 43600: train loss 1.4512, val loss 1.4152\n",
      "step 43700: train loss 1.4130, val loss 1.3884\n",
      "step 43800: train loss 1.4244, val loss 1.3743\n",
      "step 43900: train loss 1.4339, val loss 1.4210\n",
      "step 44000: train loss 1.4096, val loss 1.3924\n",
      "step 44100: train loss 1.4495, val loss 1.4592\n",
      "step 44200: train loss 1.4217, val loss 1.4275\n",
      "step 44300: train loss 1.4029, val loss 1.3988\n",
      "step 44400: train loss 1.4019, val loss 1.3963\n",
      "step 44500: train loss 1.4608, val loss 1.4584\n",
      "step 44600: train loss 1.4149, val loss 1.3941\n",
      "step 44700: train loss 1.4000, val loss 1.4167\n",
      "step 44800: train loss 1.4104, val loss 1.4416\n",
      "step 44900: train loss 1.4451, val loss 1.4203\n",
      "step 45000: train loss 1.4122, val loss 1.4284\n",
      "step 45100: train loss 1.4303, val loss 1.4667\n",
      "step 45200: train loss 1.4192, val loss 1.4459\n",
      "step 45300: train loss 1.4228, val loss 1.4087\n",
      "step 45400: train loss 1.4219, val loss 1.3887\n",
      "step 45500: train loss 1.4305, val loss 1.4085\n",
      "step 45600: train loss 1.4227, val loss 1.4256\n",
      "step 45700: train loss 1.4146, val loss 1.4153\n",
      "step 45800: train loss 1.4322, val loss 1.4184\n",
      "step 45900: train loss 1.4272, val loss 1.4019\n",
      "step 46000: train loss 1.4119, val loss 1.4145\n",
      "step 46100: train loss 1.4373, val loss 1.4154\n",
      "step 46200: train loss 1.4225, val loss 1.4146\n",
      "step 46300: train loss 1.4169, val loss 1.4061\n",
      "step 46400: train loss 1.4214, val loss 1.4081\n",
      "step 46500: train loss 1.4125, val loss 1.3865\n",
      "step 46600: train loss 1.4330, val loss 1.4019\n",
      "step 46700: train loss 1.3983, val loss 1.4145\n",
      "step 46800: train loss 1.4291, val loss 1.4227\n",
      "step 46900: train loss 1.4267, val loss 1.4322\n",
      "step 47000: train loss 1.4344, val loss 1.3972\n",
      "step 47100: train loss 1.4219, val loss 1.3891\n",
      "step 47200: train loss 1.3900, val loss 1.3934\n",
      "step 47300: train loss 1.4031, val loss 1.4219\n",
      "step 47400: train loss 1.3808, val loss 1.4156\n",
      "step 47500: train loss 1.4004, val loss 1.4051\n",
      "step 47600: train loss 1.4342, val loss 1.4070\n",
      "step 47700: train loss 1.4141, val loss 1.4331\n",
      "step 47800: train loss 1.4605, val loss 1.3924\n",
      "step 47900: train loss 1.4368, val loss 1.4411\n",
      "step 48000: train loss 1.3773, val loss 1.4132\n",
      "step 48100: train loss 1.4122, val loss 1.4064\n",
      "step 48200: train loss 1.4049, val loss 1.4222\n",
      "step 48300: train loss 1.4123, val loss 1.4470\n",
      "step 48400: train loss 1.4093, val loss 1.4142\n",
      "step 48500: train loss 1.4277, val loss 1.4090\n",
      "step 48600: train loss 1.4175, val loss 1.4039\n",
      "step 48700: train loss 1.4199, val loss 1.4046\n",
      "step 48800: train loss 1.4199, val loss 1.3993\n",
      "step 48900: train loss 1.4513, val loss 1.4334\n",
      "step 49000: train loss 1.3899, val loss 1.4227\n",
      "step 49100: train loss 1.4228, val loss 1.4092\n",
      "step 49200: train loss 1.4054, val loss 1.3802\n",
      "step 49300: train loss 1.3951, val loss 1.4146\n",
      "step 49400: train loss 1.3785, val loss 1.3776\n",
      "step 49500: train loss 1.3972, val loss 1.4080\n",
      "step 49600: train loss 1.3952, val loss 1.3788\n",
      "step 49700: train loss 1.4081, val loss 1.4024\n",
      "step 49800: train loss 1.3965, val loss 1.4035\n",
      "step 49900: train loss 1.3781, val loss 1.3753\n",
      "step 50000: train loss 1.4208, val loss 1.3828\n",
      "step 50100: train loss 1.4036, val loss 1.3950\n",
      "step 50200: train loss 1.3917, val loss 1.3737\n",
      "step 50300: train loss 1.3878, val loss 1.4279\n",
      "step 50400: train loss 1.4013, val loss 1.3862\n",
      "step 50500: train loss 1.3973, val loss 1.3900\n",
      "step 50600: train loss 1.3963, val loss 1.4212\n",
      "step 50700: train loss 1.4145, val loss 1.3856\n",
      "step 50800: train loss 1.3990, val loss 1.4013\n",
      "step 50900: train loss 1.3834, val loss 1.4106\n",
      "step 51000: train loss 1.4024, val loss 1.3881\n",
      "step 51100: train loss 1.4102, val loss 1.3795\n",
      "step 51200: train loss 1.4144, val loss 1.3650\n",
      "step 51300: train loss 1.3897, val loss 1.3841\n",
      "step 51400: train loss 1.3939, val loss 1.3992\n",
      "step 51500: train loss 1.4120, val loss 1.3949\n",
      "step 51600: train loss 1.3816, val loss 1.3964\n",
      "step 51700: train loss 1.4189, val loss 1.4072\n",
      "step 51800: train loss 1.3908, val loss 1.3812\n",
      "step 51900: train loss 1.3843, val loss 1.3939\n",
      "step 52000: train loss 1.4029, val loss 1.3818\n",
      "step 52100: train loss 1.3956, val loss 1.4088\n",
      "step 52200: train loss 1.4225, val loss 1.4134\n",
      "step 52300: train loss 1.3997, val loss 1.3751\n",
      "step 52400: train loss 1.4388, val loss 1.3940\n",
      "step 52500: train loss 1.3711, val loss 1.3549\n",
      "step 52600: train loss 1.3931, val loss 1.4157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 52700: train loss 1.4140, val loss 1.3730\n",
      "step 52800: train loss 1.3775, val loss 1.3767\n",
      "step 52900: train loss 1.3935, val loss 1.3712\n",
      "step 53000: train loss 1.4031, val loss 1.4072\n",
      "step 53100: train loss 1.4145, val loss 1.3822\n",
      "step 53200: train loss 1.4206, val loss 1.3739\n",
      "step 53300: train loss 1.3951, val loss 1.3964\n",
      "step 53400: train loss 1.3764, val loss 1.3984\n",
      "step 53500: train loss 1.4018, val loss 1.3760\n",
      "step 53600: train loss 1.4078, val loss 1.3934\n",
      "step 53700: train loss 1.3682, val loss 1.3838\n",
      "step 53800: train loss 1.4036, val loss 1.4026\n",
      "step 53900: train loss 1.3978, val loss 1.3603\n",
      "step 54000: train loss 1.4080, val loss 1.4142\n",
      "step 54100: train loss 1.3972, val loss 1.3773\n",
      "step 54200: train loss 1.3747, val loss 1.3660\n",
      "step 54300: train loss 1.3968, val loss 1.3791\n",
      "step 54400: train loss 1.4102, val loss 1.3783\n",
      "step 54500: train loss 1.3666, val loss 1.3957\n",
      "step 54600: train loss 1.4094, val loss 1.3876\n",
      "step 54700: train loss 1.3869, val loss 1.3973\n",
      "step 54800: train loss 1.3820, val loss 1.3575\n",
      "step 54900: train loss 1.4239, val loss 1.3922\n",
      "step 55000: train loss 1.3734, val loss 1.3702\n",
      "step 55100: train loss 1.4026, val loss 1.3864\n",
      "step 55200: train loss 1.3904, val loss 1.4060\n",
      "step 55300: train loss 1.3816, val loss 1.4026\n",
      "step 55400: train loss 1.3953, val loss 1.3720\n",
      "step 55500: train loss 1.3693, val loss 1.3589\n",
      "step 55600: train loss 1.3727, val loss 1.3569\n",
      "step 55700: train loss 1.3919, val loss 1.3921\n",
      "step 55800: train loss 1.3795, val loss 1.3717\n",
      "step 55900: train loss 1.3820, val loss 1.3753\n",
      "step 56000: train loss 1.3585, val loss 1.3907\n",
      "step 56100: train loss 1.3816, val loss 1.3726\n",
      "step 56200: train loss 1.3872, val loss 1.3797\n",
      "step 56300: train loss 1.3738, val loss 1.3684\n",
      "step 56400: train loss 1.3657, val loss 1.3521\n",
      "step 56500: train loss 1.3600, val loss 1.4030\n",
      "step 56600: train loss 1.3646, val loss 1.3605\n",
      "step 56700: train loss 1.3983, val loss 1.3527\n",
      "step 56800: train loss 1.4044, val loss 1.3536\n",
      "step 56900: train loss 1.3788, val loss 1.3665\n",
      "step 57000: train loss 1.3344, val loss 1.3648\n",
      "step 57100: train loss 1.3627, val loss 1.3570\n",
      "step 57200: train loss 1.4173, val loss 1.3843\n",
      "step 57300: train loss 1.3409, val loss 1.3879\n",
      "step 57400: train loss 1.3841, val loss 1.3728\n",
      "step 57500: train loss 1.3863, val loss 1.3937\n",
      "step 57600: train loss 1.3661, val loss 1.3447\n",
      "step 57700: train loss 1.3626, val loss 1.3461\n",
      "step 57800: train loss 1.3923, val loss 1.4149\n",
      "step 57900: train loss 1.3658, val loss 1.3653\n",
      "step 58000: train loss 1.3644, val loss 1.3652\n",
      "step 58100: train loss 1.3677, val loss 1.3905\n",
      "step 58200: train loss 1.3533, val loss 1.3674\n",
      "step 58300: train loss 1.3739, val loss 1.3927\n",
      "step 58400: train loss 1.3780, val loss 1.3389\n",
      "step 58500: train loss 1.3598, val loss 1.3714\n",
      "step 58600: train loss 1.3624, val loss 1.4199\n",
      "step 58700: train loss 1.3670, val loss 1.3707\n",
      "step 58800: train loss 1.3816, val loss 1.3461\n",
      "step 58900: train loss 1.3722, val loss 1.3882\n",
      "step 59000: train loss 1.3859, val loss 1.3826\n",
      "step 59100: train loss 1.3693, val loss 1.3866\n",
      "step 59200: train loss 1.3727, val loss 1.3657\n",
      "step 59300: train loss 1.3793, val loss 1.3326\n",
      "step 59400: train loss 1.3671, val loss 1.3582\n",
      "step 59500: train loss 1.3444, val loss 1.3654\n",
      "step 59600: train loss 1.3882, val loss 1.3515\n",
      "step 59700: train loss 1.3462, val loss 1.3701\n",
      "step 59800: train loss 1.3530, val loss 1.3687\n",
      "step 59900: train loss 1.3588, val loss 1.3616\n",
      "step 60000: train loss 1.4042, val loss 1.4056\n",
      "step 60100: train loss 1.3878, val loss 1.3905\n",
      "step 60200: train loss 1.3867, val loss 1.3666\n",
      "step 60300: train loss 1.3577, val loss 1.3590\n",
      "step 60400: train loss 1.3516, val loss 1.3592\n",
      "step 60500: train loss 1.3817, val loss 1.3530\n",
      "step 60600: train loss 1.3815, val loss 1.3468\n",
      "step 60700: train loss 1.3680, val loss 1.3671\n",
      "step 60800: train loss 1.3547, val loss 1.3507\n",
      "step 60900: train loss 1.3558, val loss 1.3370\n",
      "step 61000: train loss 1.3508, val loss 1.3771\n",
      "step 61100: train loss 1.3614, val loss 1.3506\n",
      "step 61200: train loss 1.3505, val loss 1.3482\n",
      "step 61300: train loss 1.3434, val loss 1.3368\n",
      "step 61400: train loss 1.3345, val loss 1.3324\n",
      "step 61500: train loss 1.3605, val loss 1.3715\n",
      "step 61600: train loss 1.3767, val loss 1.3174\n",
      "step 61700: train loss 1.3854, val loss 1.3718\n",
      "step 61800: train loss 1.3436, val loss 1.3529\n",
      "step 61900: train loss 1.4021, val loss 1.3785\n",
      "step 62000: train loss 1.3640, val loss 1.3659\n",
      "step 62100: train loss 1.3599, val loss 1.3538\n",
      "step 62200: train loss 1.3618, val loss 1.3321\n",
      "step 62300: train loss 1.3645, val loss 1.3451\n",
      "step 62400: train loss 1.3390, val loss 1.3310\n",
      "step 62500: train loss 1.3568, val loss 1.3535\n",
      "step 62600: train loss 1.3817, val loss 1.3771\n",
      "step 62700: train loss 1.3702, val loss 1.3484\n",
      "step 62800: train loss 1.3588, val loss 1.3537\n",
      "step 62900: train loss 1.3628, val loss 1.3625\n",
      "step 63000: train loss 1.3970, val loss 1.3729\n",
      "step 63100: train loss 1.3813, val loss 1.3485\n",
      "step 63200: train loss 1.3927, val loss 1.3916\n",
      "step 63300: train loss 1.3938, val loss 1.3758\n",
      "step 63400: train loss 1.3323, val loss 1.3583\n",
      "step 63500: train loss 1.3555, val loss 1.3226\n",
      "step 63600: train loss 1.3626, val loss 1.3297\n",
      "step 63700: train loss 1.3794, val loss 1.3505\n",
      "step 63800: train loss 1.3498, val loss 1.3722\n",
      "step 63900: train loss 1.3780, val loss 1.3700\n",
      "step 64000: train loss 1.3820, val loss 1.3633\n",
      "step 64100: train loss 1.3597, val loss 1.3625\n",
      "step 64200: train loss 1.3555, val loss 1.3504\n",
      "step 64300: train loss 1.3391, val loss 1.3277\n",
      "step 64400: train loss 1.3714, val loss 1.3521\n",
      "step 64500: train loss 1.3715, val loss 1.4045\n",
      "step 64600: train loss 1.3695, val loss 1.3786\n",
      "step 64700: train loss 1.3464, val loss 1.3538\n",
      "step 64800: train loss 1.3688, val loss 1.3484\n",
      "step 64900: train loss 1.3848, val loss 1.3529\n",
      "step 65000: train loss 1.3683, val loss 1.3588\n",
      "step 65100: train loss 1.4011, val loss 1.3938\n",
      "step 65200: train loss 1.3486, val loss 1.3332\n",
      "step 65300: train loss 1.3394, val loss 1.3532\n",
      "step 65400: train loss 1.3776, val loss 1.3741\n",
      "step 65500: train loss 1.3488, val loss 1.3606\n",
      "step 65600: train loss 1.3640, val loss 1.3411\n",
      "step 65700: train loss 1.3775, val loss 1.3777\n",
      "step 65800: train loss 1.3566, val loss 1.3811\n",
      "step 65900: train loss 1.3736, val loss 1.3853\n",
      "step 66000: train loss 1.3858, val loss 1.3750\n",
      "step 66100: train loss 1.3537, val loss 1.3225\n",
      "step 66200: train loss 1.3362, val loss 1.3452\n",
      "step 66300: train loss 1.3362, val loss 1.3715\n",
      "step 66400: train loss 1.3711, val loss 1.3798\n",
      "step 66500: train loss 1.3799, val loss 1.3599\n",
      "step 66600: train loss 1.3466, val loss 1.3396\n",
      "step 66700: train loss 1.3464, val loss 1.3685\n",
      "step 66800: train loss 1.3823, val loss 1.3617\n",
      "step 66900: train loss 1.3580, val loss 1.3290\n",
      "step 67000: train loss 1.3421, val loss 1.3584\n",
      "step 67100: train loss 1.3806, val loss 1.3394\n",
      "step 67200: train loss 1.3367, val loss 1.3716\n",
      "step 67300: train loss 1.3547, val loss 1.3429\n",
      "step 67400: train loss 1.3591, val loss 1.3315\n",
      "step 67500: train loss 1.3522, val loss 1.3170\n",
      "step 67600: train loss 1.3574, val loss 1.3517\n",
      "step 67700: train loss 1.3614, val loss 1.3690\n",
      "step 67800: train loss 1.3214, val loss 1.3599\n",
      "step 67900: train loss 1.3614, val loss 1.3197\n",
      "step 68000: train loss 1.3259, val loss 1.3310\n",
      "step 68100: train loss 1.3441, val loss 1.3400\n",
      "step 68200: train loss 1.3598, val loss 1.3667\n",
      "step 68300: train loss 1.3429, val loss 1.3489\n",
      "step 68400: train loss 1.3460, val loss 1.3359\n",
      "step 68500: train loss 1.3714, val loss 1.3503\n",
      "step 68600: train loss 1.3767, val loss 1.3693\n",
      "step 68700: train loss 1.3406, val loss 1.3692\n",
      "step 68800: train loss 1.3958, val loss 1.3506\n",
      "step 68900: train loss 1.3669, val loss 1.3533\n",
      "step 69000: train loss 1.3688, val loss 1.3760\n",
      "step 69100: train loss 1.3072, val loss 1.3396\n",
      "step 69200: train loss 1.3379, val loss 1.3552\n",
      "step 69300: train loss 1.3204, val loss 1.3446\n",
      "step 69400: train loss 1.3361, val loss 1.3388\n",
      "step 69500: train loss 1.3412, val loss 1.3536\n",
      "step 69600: train loss 1.3483, val loss 1.3544\n",
      "step 69700: train loss 1.3604, val loss 1.3495\n",
      "step 69800: train loss 1.3760, val loss 1.3195\n",
      "step 69900: train loss 1.3272, val loss 1.3419\n",
      "step 70000: train loss 1.3620, val loss 1.3588\n",
      "step 70100: train loss 1.3547, val loss 1.3431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 70200: train loss 1.3417, val loss 1.3492\n",
      "step 70300: train loss 1.3585, val loss 1.3385\n",
      "step 70400: train loss 1.3626, val loss 1.3206\n",
      "step 70500: train loss 1.3507, val loss 1.3283\n",
      "step 70600: train loss 1.3314, val loss 1.3633\n",
      "step 70700: train loss 1.3437, val loss 1.3665\n",
      "step 70800: train loss 1.3295, val loss 1.3222\n",
      "step 70900: train loss 1.3575, val loss 1.3439\n",
      "step 71000: train loss 1.3336, val loss 1.3411\n",
      "step 71100: train loss 1.3314, val loss 1.3451\n",
      "step 71200: train loss 1.3427, val loss 1.3478\n",
      "step 71300: train loss 1.3272, val loss 1.3262\n",
      "step 71400: train loss 1.3111, val loss 1.3102\n",
      "step 71500: train loss 1.3113, val loss 1.3374\n",
      "step 71600: train loss 1.3494, val loss 1.3708\n",
      "step 71700: train loss 1.3314, val loss 1.3456\n",
      "step 71800: train loss 1.3626, val loss 1.3641\n",
      "step 71900: train loss 1.3354, val loss 1.3300\n",
      "step 72000: train loss 1.3831, val loss 1.3381\n",
      "step 72100: train loss 1.3835, val loss 1.3396\n",
      "step 72200: train loss 1.3537, val loss 1.3242\n",
      "step 72300: train loss 1.3646, val loss 1.3070\n",
      "step 72400: train loss 1.3196, val loss 1.3226\n",
      "step 72500: train loss 1.3361, val loss 1.3318\n",
      "step 72600: train loss 1.3395, val loss 1.3393\n",
      "step 72700: train loss 1.3467, val loss 1.3099\n",
      "step 72800: train loss 1.3524, val loss 1.3064\n",
      "step 72900: train loss 1.3260, val loss 1.3073\n",
      "step 73000: train loss 1.3466, val loss 1.3303\n",
      "step 73100: train loss 1.3389, val loss 1.3420\n",
      "step 73200: train loss 1.4077, val loss 1.3703\n",
      "step 73300: train loss 1.3656, val loss 1.3175\n",
      "step 73400: train loss 1.3283, val loss 1.3244\n",
      "step 73500: train loss 1.3166, val loss 1.3277\n",
      "step 73600: train loss 1.3450, val loss 1.3464\n",
      "step 73700: train loss 1.3264, val loss 1.3411\n",
      "step 73800: train loss 1.3267, val loss 1.3342\n",
      "step 73900: train loss 1.3252, val loss 1.3379\n",
      "step 74000: train loss 1.3265, val loss 1.3351\n",
      "step 74100: train loss 1.3670, val loss 1.3479\n",
      "step 74200: train loss 1.3399, val loss 1.3192\n",
      "step 74300: train loss 1.3281, val loss 1.3554\n",
      "step 74400: train loss 1.3523, val loss 1.3438\n",
      "step 74500: train loss 1.3686, val loss 1.3266\n",
      "step 74600: train loss 1.3723, val loss 1.3280\n",
      "step 74700: train loss 1.2937, val loss 1.2891\n",
      "step 74800: train loss 1.3444, val loss 1.2978\n",
      "step 74900: train loss 1.3357, val loss 1.3260\n",
      "step 75000: train loss 1.3233, val loss 1.3381\n",
      "step 75100: train loss 1.3022, val loss 1.3344\n",
      "step 75200: train loss 1.3522, val loss 1.3102\n",
      "step 75300: train loss 1.3411, val loss 1.3385\n",
      "step 75400: train loss 1.3478, val loss 1.3428\n",
      "step 75500: train loss 1.3408, val loss 1.3256\n",
      "step 75600: train loss 1.3262, val loss 1.2881\n",
      "step 75700: train loss 1.3388, val loss 1.3316\n",
      "step 75800: train loss 1.3334, val loss 1.3537\n",
      "step 75900: train loss 1.3553, val loss 1.3225\n",
      "step 76000: train loss 1.2964, val loss 1.3565\n",
      "step 76100: train loss 1.3343, val loss 1.3170\n",
      "step 76200: train loss 1.3217, val loss 1.3107\n",
      "step 76300: train loss 1.3531, val loss 1.3329\n",
      "step 76400: train loss 1.3397, val loss 1.2948\n",
      "step 76500: train loss 1.3536, val loss 1.3105\n",
      "step 76600: train loss 1.3289, val loss 1.3087\n",
      "step 76700: train loss 1.3442, val loss 1.2957\n",
      "step 76800: train loss 1.3441, val loss 1.3280\n",
      "step 76900: train loss 1.3703, val loss 1.3327\n",
      "step 77000: train loss 1.3123, val loss 1.3003\n",
      "step 77100: train loss 1.3328, val loss 1.3547\n",
      "step 77200: train loss 1.3155, val loss 1.3237\n",
      "step 77300: train loss 1.3296, val loss 1.3361\n",
      "step 77400: train loss 1.3518, val loss 1.3200\n",
      "step 77500: train loss 1.3388, val loss 1.3151\n",
      "step 77600: train loss 1.3576, val loss 1.3175\n",
      "step 77700: train loss 1.3218, val loss 1.3179\n",
      "step 77800: train loss 1.3434, val loss 1.3446\n",
      "step 77900: train loss 1.3263, val loss 1.2924\n",
      "step 78000: train loss 1.3197, val loss 1.3132\n",
      "step 78100: train loss 1.3441, val loss 1.3219\n",
      "step 78200: train loss 1.3252, val loss 1.2959\n",
      "step 78300: train loss 1.3393, val loss 1.3587\n",
      "step 78400: train loss 1.3210, val loss 1.3291\n",
      "step 78500: train loss 1.3510, val loss 1.3513\n",
      "step 78600: train loss 1.2998, val loss 1.3139\n",
      "step 78700: train loss 1.3215, val loss 1.3194\n",
      "step 78800: train loss 1.3384, val loss 1.2885\n",
      "step 78900: train loss 1.2897, val loss 1.2968\n",
      "step 79000: train loss 1.3234, val loss 1.3125\n",
      "step 79100: train loss 1.3697, val loss 1.3162\n",
      "step 79200: train loss 1.3493, val loss 1.3192\n",
      "step 79300: train loss 1.3053, val loss 1.2757\n",
      "step 79400: train loss 1.3286, val loss 1.3218\n",
      "step 79500: train loss 1.3534, val loss 1.3253\n",
      "step 79600: train loss 1.3467, val loss 1.3667\n",
      "step 79700: train loss 1.2976, val loss 1.3178\n",
      "step 79800: train loss 1.3529, val loss 1.3409\n",
      "step 79900: train loss 1.3366, val loss 1.3046\n",
      "step 80000: train loss 1.3374, val loss 1.3280\n",
      "step 80100: train loss 1.3315, val loss 1.3317\n",
      "step 80200: train loss 1.2974, val loss 1.2871\n",
      "step 80300: train loss 1.3424, val loss 1.3691\n",
      "step 80400: train loss 1.3126, val loss 1.2986\n",
      "step 80500: train loss 1.3422, val loss 1.3260\n",
      "step 80600: train loss 1.2963, val loss 1.3090\n",
      "step 80700: train loss 1.3332, val loss 1.3227\n",
      "step 80800: train loss 1.3373, val loss 1.2939\n",
      "step 80900: train loss 1.3262, val loss 1.3125\n",
      "step 81000: train loss 1.3170, val loss 1.3232\n",
      "step 81100: train loss 1.3016, val loss 1.3102\n",
      "step 81200: train loss 1.3373, val loss 1.3387\n",
      "step 81300: train loss 1.3288, val loss 1.3437\n",
      "step 81400: train loss 1.3438, val loss 1.3160\n",
      "step 81500: train loss 1.3243, val loss 1.3225\n",
      "step 81600: train loss 1.3241, val loss 1.3295\n",
      "step 81700: train loss 1.3308, val loss 1.3234\n",
      "step 81800: train loss 1.3204, val loss 1.3087\n",
      "step 81900: train loss 1.3104, val loss 1.2870\n",
      "step 82000: train loss 1.3439, val loss 1.3507\n",
      "step 82100: train loss 1.3492, val loss 1.3143\n",
      "step 82200: train loss 1.3338, val loss 1.3410\n",
      "step 82300: train loss 1.3359, val loss 1.3080\n",
      "step 82400: train loss 1.3200, val loss 1.3192\n",
      "step 82500: train loss 1.2995, val loss 1.3024\n",
      "step 82600: train loss 1.3586, val loss 1.2949\n",
      "step 82700: train loss 1.3006, val loss 1.3324\n",
      "step 82800: train loss 1.3294, val loss 1.3156\n",
      "step 82900: train loss 1.3335, val loss 1.3117\n",
      "step 83000: train loss 1.3439, val loss 1.3332\n",
      "step 83100: train loss 1.3200, val loss 1.3209\n",
      "step 83200: train loss 1.3181, val loss 1.3028\n",
      "step 83300: train loss 1.3290, val loss 1.3056\n",
      "step 83400: train loss 1.3225, val loss 1.3115\n",
      "step 83500: train loss 1.2941, val loss 1.2899\n",
      "step 83600: train loss 1.3375, val loss 1.3288\n",
      "step 83700: train loss 1.3080, val loss 1.2951\n",
      "step 83800: train loss 1.2969, val loss 1.3045\n",
      "step 83900: train loss 1.3278, val loss 1.2774\n",
      "step 84000: train loss 1.3101, val loss 1.3180\n",
      "step 84100: train loss 1.3283, val loss 1.3143\n",
      "step 84200: train loss 1.3229, val loss 1.3387\n",
      "step 84300: train loss 1.3118, val loss 1.2966\n",
      "step 84400: train loss 1.2956, val loss 1.3005\n",
      "step 84500: train loss 1.3021, val loss 1.2998\n",
      "step 84600: train loss 1.2891, val loss 1.3172\n",
      "step 84700: train loss 1.3231, val loss 1.3482\n",
      "step 84800: train loss 1.3165, val loss 1.3006\n",
      "step 84900: train loss 1.2913, val loss 1.3005\n",
      "step 85000: train loss 1.3203, val loss 1.3478\n",
      "step 85100: train loss 1.3078, val loss 1.2991\n",
      "step 85200: train loss 1.3202, val loss 1.3314\n",
      "step 85300: train loss 1.3269, val loss 1.3299\n",
      "step 85400: train loss 1.3418, val loss 1.3092\n",
      "step 85500: train loss 1.3133, val loss 1.2947\n",
      "step 85600: train loss 1.3051, val loss 1.3128\n",
      "step 85700: train loss 1.2569, val loss 1.3069\n",
      "step 85800: train loss 1.2963, val loss 1.3018\n",
      "step 85900: train loss 1.3080, val loss 1.3129\n",
      "step 86000: train loss 1.3371, val loss 1.3017\n",
      "step 86100: train loss 1.2808, val loss 1.3070\n",
      "step 86200: train loss 1.3119, val loss 1.2868\n",
      "step 86300: train loss 1.3188, val loss 1.3054\n",
      "step 86400: train loss 1.2851, val loss 1.3165\n",
      "step 86500: train loss 1.3000, val loss 1.3308\n",
      "step 86600: train loss 1.3264, val loss 1.2894\n",
      "step 86700: train loss 1.3106, val loss 1.2736\n",
      "step 86800: train loss 1.3208, val loss 1.3031\n",
      "step 86900: train loss 1.2978, val loss 1.3138\n",
      "step 87000: train loss 1.2961, val loss 1.3260\n",
      "step 87100: train loss 1.3072, val loss 1.3032\n",
      "step 87200: train loss 1.3408, val loss 1.3060\n",
      "step 87300: train loss 1.3306, val loss 1.3511\n",
      "step 87400: train loss 1.3233, val loss 1.3195\n",
      "step 87500: train loss 1.3008, val loss 1.3007\n",
      "step 87600: train loss 1.2932, val loss 1.2951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 87700: train loss 1.3089, val loss 1.3174\n",
      "step 87800: train loss 1.3417, val loss 1.3133\n",
      "step 87900: train loss 1.3250, val loss 1.3049\n",
      "step 88000: train loss 1.3105, val loss 1.3031\n",
      "step 88100: train loss 1.3516, val loss 1.2725\n",
      "step 88200: train loss 1.3306, val loss 1.2923\n",
      "step 88300: train loss 1.2735, val loss 1.3192\n",
      "step 88400: train loss 1.2832, val loss 1.3081\n",
      "step 88500: train loss 1.3173, val loss 1.2815\n",
      "step 88600: train loss 1.3012, val loss 1.2878\n",
      "step 88700: train loss 1.3043, val loss 1.3041\n",
      "step 88800: train loss 1.3266, val loss 1.3233\n",
      "step 88900: train loss 1.2769, val loss 1.2914\n",
      "step 89000: train loss 1.3344, val loss 1.3040\n",
      "step 89100: train loss 1.3504, val loss 1.3213\n",
      "step 89200: train loss 1.3052, val loss 1.2986\n",
      "step 89300: train loss 1.3167, val loss 1.2667\n",
      "step 89400: train loss 1.3257, val loss 1.3174\n",
      "step 89500: train loss 1.3346, val loss 1.3151\n",
      "step 89600: train loss 1.3012, val loss 1.2846\n",
      "step 89700: train loss 1.3088, val loss 1.2993\n",
      "step 89800: train loss 1.2801, val loss 1.2989\n",
      "step 89900: train loss 1.3090, val loss 1.3013\n",
      "step 90000: train loss 1.2897, val loss 1.3273\n",
      "step 90100: train loss 1.3153, val loss 1.2967\n",
      "step 90200: train loss 1.3126, val loss 1.3158\n",
      "step 90300: train loss 1.2963, val loss 1.2874\n",
      "step 90400: train loss 1.2949, val loss 1.2880\n",
      "step 90500: train loss 1.2905, val loss 1.2969\n",
      "step 90600: train loss 1.2902, val loss 1.2917\n",
      "step 90700: train loss 1.2941, val loss 1.3014\n",
      "step 90800: train loss 1.2803, val loss 1.3231\n",
      "step 90900: train loss 1.3104, val loss 1.3106\n",
      "step 91000: train loss 1.3247, val loss 1.3170\n",
      "step 91100: train loss 1.3449, val loss 1.3213\n",
      "step 91200: train loss 1.3111, val loss 1.3315\n",
      "step 91300: train loss 1.3334, val loss 1.3285\n",
      "step 91400: train loss 1.3119, val loss 1.3037\n",
      "step 91500: train loss 1.2900, val loss 1.2915\n",
      "step 91600: train loss 1.3228, val loss 1.2981\n",
      "step 91700: train loss 1.3026, val loss 1.3059\n",
      "step 91800: train loss 1.3296, val loss 1.3572\n",
      "step 91900: train loss 1.3024, val loss 1.2807\n",
      "step 92000: train loss 1.3165, val loss 1.2831\n",
      "step 92100: train loss 1.3127, val loss 1.2993\n",
      "step 92200: train loss 1.3171, val loss 1.3266\n",
      "step 92300: train loss 1.2808, val loss 1.2556\n",
      "step 92400: train loss 1.3063, val loss 1.3103\n",
      "step 92500: train loss 1.3040, val loss 1.2825\n",
      "step 92600: train loss 1.2797, val loss 1.2907\n",
      "step 92700: train loss 1.3025, val loss 1.2856\n",
      "step 92800: train loss 1.3212, val loss 1.2841\n",
      "step 92900: train loss 1.3135, val loss 1.3014\n",
      "step 93000: train loss 1.3343, val loss 1.3263\n",
      "step 93100: train loss 1.3238, val loss 1.3068\n",
      "step 93200: train loss 1.3091, val loss 1.2921\n",
      "step 93300: train loss 1.2965, val loss 1.3099\n",
      "step 93400: train loss 1.2856, val loss 1.2857\n",
      "step 93500: train loss 1.3197, val loss 1.2910\n",
      "step 93600: train loss 1.3079, val loss 1.2692\n",
      "step 93700: train loss 1.2843, val loss 1.2964\n",
      "step 93800: train loss 1.2694, val loss 1.3023\n",
      "step 93900: train loss 1.2879, val loss 1.3272\n",
      "step 94000: train loss 1.2721, val loss 1.3192\n",
      "step 94100: train loss 1.2864, val loss 1.2949\n",
      "step 94200: train loss 1.2758, val loss 1.3069\n",
      "step 94300: train loss 1.2897, val loss 1.3013\n",
      "step 94400: train loss 1.3105, val loss 1.2856\n",
      "step 94500: train loss 1.2587, val loss 1.2987\n",
      "step 94600: train loss 1.2717, val loss 1.3457\n",
      "step 94700: train loss 1.3011, val loss 1.3032\n",
      "step 94800: train loss 1.3180, val loss 1.3352\n",
      "step 94900: train loss 1.3062, val loss 1.3074\n",
      "step 95000: train loss 1.3051, val loss 1.2961\n",
      "step 95100: train loss 1.3013, val loss 1.2979\n",
      "step 95200: train loss 1.2863, val loss 1.2974\n",
      "step 95300: train loss 1.2873, val loss 1.2786\n",
      "step 95400: train loss 1.2887, val loss 1.2920\n",
      "step 95500: train loss 1.3518, val loss 1.3548\n",
      "step 95600: train loss 1.3161, val loss 1.2830\n",
      "step 95700: train loss 1.3174, val loss 1.3175\n",
      "step 95800: train loss 1.2792, val loss 1.2992\n",
      "step 95900: train loss 1.3122, val loss 1.2991\n",
      "step 96000: train loss 1.2693, val loss 1.2927\n",
      "step 96100: train loss 1.2797, val loss 1.2786\n",
      "step 96200: train loss 1.3002, val loss 1.2742\n",
      "step 96300: train loss 1.3403, val loss 1.2577\n",
      "step 96400: train loss 1.2936, val loss 1.2732\n",
      "step 96500: train loss 1.2962, val loss 1.2958\n",
      "step 96600: train loss 1.2704, val loss 1.2938\n",
      "step 96700: train loss 1.3036, val loss 1.2677\n",
      "step 96800: train loss 1.3030, val loss 1.2800\n",
      "step 96900: train loss 1.2758, val loss 1.2808\n",
      "step 97000: train loss 1.3225, val loss 1.2872\n",
      "step 97100: train loss 1.2700, val loss 1.2884\n",
      "step 97200: train loss 1.3224, val loss 1.2857\n",
      "step 97300: train loss 1.3019, val loss 1.3124\n",
      "step 97400: train loss 1.2832, val loss 1.2947\n",
      "step 97500: train loss 1.3168, val loss 1.2974\n",
      "step 97600: train loss 1.3158, val loss 1.3131\n",
      "step 97700: train loss 1.3034, val loss 1.2679\n",
      "step 97800: train loss 1.3084, val loss 1.3222\n",
      "step 97900: train loss 1.3265, val loss 1.3070\n",
      "step 98000: train loss 1.2996, val loss 1.3007\n",
      "step 98100: train loss 1.3234, val loss 1.3026\n",
      "step 98200: train loss 1.3210, val loss 1.3007\n",
      "step 98300: train loss 1.3143, val loss 1.2834\n",
      "step 98400: train loss 1.2984, val loss 1.2798\n",
      "step 98500: train loss 1.2804, val loss 1.2737\n",
      "step 98600: train loss 1.2753, val loss 1.2882\n",
      "step 98700: train loss 1.3395, val loss 1.2968\n",
      "step 98800: train loss 1.2942, val loss 1.3119\n",
      "step 98900: train loss 1.2910, val loss 1.2772\n",
      "step 99000: train loss 1.2698, val loss 1.3106\n",
      "step 99100: train loss 1.2796, val loss 1.2713\n",
      "step 99200: train loss 1.3301, val loss 1.2851\n",
      "step 99300: train loss 1.2924, val loss 1.2948\n",
      "step 99400: train loss 1.3346, val loss 1.2943\n",
      "step 99500: train loss 1.2650, val loss 1.2692\n",
      "step 99600: train loss 1.2940, val loss 1.2665\n",
      "step 99700: train loss 1.2947, val loss 1.2558\n",
      "step 99800: train loss 1.3011, val loss 1.2707\n",
      "step 99900: train loss 1.3106, val loss 1.3105\n",
      "step 99999: train loss 1.2846, val loss 1.2847\n",
      "\n",
      "MVPNGCGAQGPPPGSSSCSSQPEGPSIRVPPSRSAIRIMPLIKMGGNRTVRRKVHQRQSLRLSDTQVVLPLVRPHRSFCASAVCSGQLLPSEVTIQPSFQAILFASESFREGPHCSLTGALAVADYCGIPPTRLSCSEGYHLYQLLIAASEKAIKFPDGKKYYRSNLNGEIFL NC(=O)N[C@H](Cc2c[nH]c3cccnc34)[N+]C(=O)c2ccc3OC)c3N)ccc2N1\n",
      "MAFLSTLQLYILRAGHPTNVAPGKNTEGADNGSAGKAVIAMCVKGNGHNKTTSNAVLKEGQTFGAVEQDVEEKGYTLGQGMEEKIMMLTGRSPTFFKDWFWLFLNGKGSIGVTIQVITDSIKAFANYLIYLGVLNLAVVCRQLKNALNEKNNGGNEEAIEDWAGVTDLGADRPIDSLTCIANSAANYERFCITLLSEYLSVRARIGGGKGVYTKLVVLVKGGERMTQMLPGSMHHNSDLFGKENITEVVQGQEEIGTLLKWNFRGHATSVLTTIIILQQILDLWLLHLWAPQDRNADPGCAGKEQLGLFARLALPLIFILYTIGIYCRQAKAKMHFNPQTFPCTLFEKKATILGNQEQVDASNSTQPLPLQNVNGVLRTVPDLLLPVIQLHSDVKSPGGQGPVKLAGISRLYRDVGLLILLGEDLERHTRECQSRPHAYREMTCLPVSGELSSDIPRTIESDQSPLENSVPGQRTVELFIFFMGQDFKLARQTLSAYMRAKLIYLSLRRSGEHSTLFQTKV Cc1nnc(oc1N)-c1ncc(cc1)S(=O)(=O)NNC(=O)c1ccc(C)cc1NC(=O)COCc1c[nH]cc(C(=O)OC)[C@@]2(CC1)C(=O)N[C@@H](Cc1ccccc1)C(=O)N1CCc1ccccc1N\n",
      "MDPLNLSWNEFTAGSQIFHNLKCFTGFRGFWEYSDSNIEEVHAALGCKDKYPYYSQYVRASLKLVKNLFPSVFFILNIHRTNIYFIPLRYLLYEWATEEFLARSYLSYKRNQVG\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 100000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open(\"C:\\\\Users\\\\ncbir\\\\Desktop\\\\seqsmile.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fjjvMifYZf7x"
   },
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\ncbir\\\\Desktop\\\\ms.pt\"\n",
    "torch.save(m.state_dict(), path)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
